\documentclass[a4paper,12pt]{article}

\usepackage{url}
\usepackage{epsfig}
\usepackage{graphics}
\usepackage{fancyhdr}
\usepackage{indentfirst}
\usepackage{hyperref}

\graphicspath{{pictures/}}

\title{Natural Language Generation using Hidden Markov Model}
\author{\hspace*{-0.5cm}\begin{tabular}{cccc}
Akash Kumar Dhaka & Dario Vidas & Stefan Annell \\
1990-02-26 & 1992-06-24 & 1987-09-30 \\
akashd@kth.se & vidas@kth.se & steann@kth.se \\
\includegraphics[width=0.13\linewidth]{akash} & 
\includegraphics[width=0.13\linewidth]{dario} & 
\includegraphics[width=0.13\linewidth]{stefan} 
\end{tabular}} 
% Normally there will not be any pictures but we want
% these so that we can connect faces to names in the course
% We also want birthdates so that we can tell people with the same
% name apart
\date{}

\pagestyle{fancy}
\setlength{\headheight}{15pt}
\fancyhf{}
\lhead{DD2380 ai14} % DO NOT REMOVE!!!!
\rhead{A.K. Dhaka, D. Vidas, S. Annell} %% UPDATE WITH YOUR NAMES

\begin{document}

\nocite{*}

\maketitle
\thispagestyle{fancy}

\begin{abstract}
Natural Language Generation (NLG) is a Natural Language Processing (NLP) task of
generating natural language. The problem is well known and well researched, but
the field is far from being fully explored. In this paper, we propose an idea of
building a text generator based on Hidden Markov Model (HMM). HMM allows us to
append words in a sentence that depend on a whole sequence until that point,
unlike other methods, such as n-gram, which use only a certain number of words
to predict the next one. . This type of text generators may outperform simple ones
and match the performance of a more complex generators.
\end{abstract}



\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}
Natural Language Generation(NLG) is the field of generating natural language for
a computer from a knowledge base or some information that the system can
understand. Some fields where the NLG is beeing applied are weather
forecast generation, summarize statistical data extracted from a database and
describing chains of reasoning performed by a system~\cite{nlgsystem01}.

This paper is about designing, implementing and comparing different methods of
generating random sentences when fed in by a specific corpus. The generated 
sentences should be as correct as possible grammatically and semantically.
Which means that Natural Language Processing (NLP) needs to be implemented
for building the corpus and tagging / structuring the words, allowing our system
to form sentences that are meaningful and semantically as correct as possible.
The corpora used were of different sizes to test our system properly.

This is very interesting field of research and a good topic to study because it
incorporates several complicated tasks that are useful in wide variety of
applications. First of all, it is very difficult for a computer to understand
the content of a text, there has been a lot of work done in this field. 
It is also hard for a computer to construct sentences that are correct
semantically because of the complexity of a natural languages, such as English
as mentioned in \cite{nlgScratch}. The evaluation of how correct a sentence is 
also very challenging and a field in its own. This fact raises another problem, 
which is the infinite number of combinations of words. Furthermore, the 
aggravating circumstance that the language (both dictionary and the grammar)
 is continuously evolving, makes it even harder to define a specific set of rules 
that apply all the time, considering that our datasets (news) are written by numerous authors and in
different time instants. NLG systems broadly have three subcomponents:text
planning, sentence planning and surface realisation.
In text planning, content and target of the text are determined, During sentence
planning, syntactic and lexical means are ascertained whereas in realisation, 
actual words are strung together to give form to the basic structure which comes out
of sentence planning. 
It is also preferred if the results are benchmarked in some systematical way,
either manually (which is not preferred), semi-manually where the system might
give suggestions and the operator help the computer or it is done completely
automated (which is also quite hard).


\subsection{Contribution}
It is very hard for a computer to communicate with a human since the language is
very complex and it is hard to replicate and the language is one of the big
borders between computers and humans which makes this field of research very
interesting. The method that we are presenting is a new and interesting way to
generate text focused on a topic using Hidden Markov Models (HMM). We used
three variants of HMM for this taks- unsupervised HMM, supervised HMM and 
chunked supervised HMM. We also generated sentenced by trigrams and evaluated
the performance of all the generators with human written sentences.

\subsection{Outline}
In Section~\ref{sec:relwork} previous work is recognized and the papers we have
researched to support our work is presented. Section~\ref{sec:method} contains
an overview on how we are going to solve the problem, and why we are choosing a
specific method and also how we are implementing the system. The result of the
implementation is presented in Section~\ref{sec:exps} and the whole paper is
summarized and concluded in Section~\ref{sec:summary}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related work}
\label{sec:relwork}

The field of NLG and NLP are two very large fields that are subfields of
artificial intelligence and computational linguistics. A lot of research has
been done in these fields, although the fields are far from fully explored. Some
of the earliest work in this field was performed in the 1970's by people like
Goldman and Davey, where they helped to define the main problems in NLG 
as explained in \cite[p.~19-20]{buildingNLG}, where they also mention that NLG
started to be researched allot during 1980's by people like McKeown and Appelt
that had a big impact on how the subsequent research was going to be performed on NLG.
The field had a big increase in the 1990's where allot of real-world applications were made,
like the weather forecast system FOG.

The paper Building Applied Natural Language Generation
Systems~\cite{nlgsystem01} is about the architecture and how to structure your
NLG system when designing it. They describe six main tasks that needs to be
performed when generating a text; content determination, discourse planning,
sentence aggregation, lexicalization, referring expression generation and
linguistic realisation. They also discuss the sub components the system
needs to design and the algorithms, using which the different subproblems can be
solved. The main use of this paper is the structure of the generation part of
the system, but also suggestions and pointers on how the arising problems can be
solved.

Usage of HMMs in text generation is mentioned in~\cite{hmmnlg} where the model
is used in a task-oriented environment. Authors describe HMM generation process
to be very similar to POS (part of speech) tagging. States represent words, and
state sequences, word phrases. The model is designed from the corpus data
using ABL algorithm and it learns using the Baum-Welch algorithm.

These papers  ~\cite{poseval} ~\cite{bleueval} researched some different methods for evaluating Natural Language Generation systems, by comparing automated methods and human evaluation methods. Where  ~\cite{bleueval} propose methods like Bleu, NIST and Rogue which looks at the words of the generated sentence, and compares it to a reference. ~\cite{poseval} Suggests a pos tagging method, that breaks up the sentences into POS tags, and compares it to a big reference corpus.
They both discuss the pros and cons of the evaluation systems and suggests that a combination of automated methods should be used, since the different automated methods covers different parts of the text.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{My method}
\label{sec:method}

\subsection{Hidden Markov Models}

n-gram models have been used for the task of NLG~\cite{nlgngram}. n-gram models use the
statistical property of co-occurrances to calculate the most likely word to
appear given previous (n-1) grams. Intuition would tell us that a higher n gram
model will perform better than a lower one as the samples will be fewer and are
more likely to be semantically correct but will be much more computationally
intensive. We have used trigrams as one of the generation techniques, But there 
are  also some problems with n-gram models.One of them 
is of the sparse data combinations as n becomes greater. The other problem which will be 
verified in the results section is that of non-randomness. Trigram tends to 
pick full sentences out of the corpus and that is where HMMs perform much
better over trigrams. This problem with ngrams gets aggravated if the training
corpus is small in size. 


We propose to use HMMs for the surface generalisation task (how to say). HMMs
have been used with very good results in the fields of speech recognition and
also in NLU (Natural Language Understanding)~\cite{hmmsr}. The idea of HMM in this
work is todo something like the inverse of what POS Tagging does. In POS Tagging, an input
string of words is mapped to a hidden sequence of semantic POS tags. We used
the default POS tagger of the nltk package. The POS Tags then can serve as the
observations in the HMM model and the words as the hidden states.  With the
different candidates for information, they can be trained by a HMM model and
then evaluated by a function which will select the best candidate out of them.
With the different candidates for information, they can be trained by a HMM
model and then evaluated by a function which will select the best candidate
out of them.

In the supervised HMMs, the POS tags are used as the states and the words of
the corpus are treated as the emissions. NLTK's default POS Tagger was used 
for evaluating the POS tags in a sentence. The state-transition matrix and 
emission matrix are then calculated with them.
In the chunked HMMs, we try to enrich the emissions with a bit more information
about the semantics of a sentence and thus add chunks with POS tags as emissions.




\subsection {Performance measures for experiments}

As in other scientific fields, it is crucial to test how well our systems,
modules and algorithms work. This type of performance measure is called
evaluation. There are three techniques of evaluating NLG systems: task-based,
human ratings and metrics~\cite{evalnlg}. In our project we are going to use
human ratings and metric evaluations.

Evaluation gives as an output the ``goodness'' of a sentence or a text. The
``goodness'' is defined by three characteristics: fluency, adequateness and
readability~\cite{evalmethods}. It is often a difficult task to assess each
characteristic.
Human ratings are based on opinions of individuals rating the given the given
text. These types of evaluations are subjective and hard to measure. But the
main advantage is the that the natural language is made for interactions between
people and we can evaluate texts more accurately than machines. In our project
we will spend a certain amount of time evaluating our models ourselves and with
the help of others to reduce the bias.

Metric evaluations are based on software evaluations of generated text. They are
more objective than human ratings, but also less reliable. Most evaluators
compare generated text with texts written by people to compute the score. In our
project, we used three evaluators- BLEU, Linguitic checker and POS sequence checker~\cite{autoeval}.
ADD TEXT ABOUT BLEU HERE STEFAN!!
For evaluating the grammatical correctness of the generated sentence, we used the
open source tool LANGUAGE TOOL. This software is quite clean and robust. It gives 
all the possible errors and their description when it is fed with a sentence. The 
goodness of a sentence can be evaluated by using the formula: 

The third test used for evaluation is the POS tag sequence checker. This measure 
gives us a good estimate of how correct the sentence is correct semantically.

\subsection {Measure success of the project work or approach}

The main task of the project is to successfully implement HMM text generator.
Besides HMM text generator, we are also going to implement simple n-gram
generators such as bigram and trigram. To measure success of the project
approach, we are going to use performance measures listed above, as well as
comparison between HMM and n-gram generator. If our hypothesis proves to be
true, HMM will perform better than bigram and trigram.
\subsection {Measure the progress during the course of\\ the project}

The project is divided into several stages. In the beginning, we will set the
hypothesis and elaborate details regarding the same. By analyzing collected
research papers, we will make decisions regarding implementation details,
improvements and alterations, and also define performance expectations of our
approach.

Second stage is the implementation. During this stage, we will implement bigram,
trigram and HMM text generators (and genetic algorithm for HMM optimization if
needed). In the same time we will also test our application to ensure algorithm
correctness.

Finally, we will run experiments (evaluations) mentioned above and alter
parameters accordingly to achieve better results.

During the course of the project, we will record our progress based on the
project plan described above. Prediction of the time consumption will be made
for each of the stages. Using those predictions, we will adjust the pace of the
project progress for the purpose of completing the task in time.

\subsection{Implementation}
\label{sec:impl}

Implementation will be done shortly.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental results}
\label{sec:exps}

The result of our experiments, will be written when we have done the implementation.


\subsection{Experiemntal setup}

The system design will be presented when the setup is done.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary and Conclusions}
\label{sec:summary}
To be written later.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{ref}
\bibliographystyle{plain}


\end{document}