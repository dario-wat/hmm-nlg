\documentclass[a4paper,12pt]{article}

\usepackage{url}
\usepackage{epsfig}
\usepackage{graphics}
\usepackage{fancyhdr}
\usepackage{indentfirst}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}

\graphicspath{{pictures/}}

\title{A Comparison of Trigram and\\ Hidden Markov Model Text Generators}
\author{\hspace*{-0.5cm}\begin{tabular}{cccc}
Akash Kumar Dhaka & Dario Vidas & Stefan Annell \\
1990-02-26 & 1992-06-24 & 1987-09-30 \\
akashd@kth.se & vidas@kth.se & steann@kth.se \\
\includegraphics[width=0.13\linewidth]{akash} & 
\includegraphics[width=0.13\linewidth]{dario} & 
\includegraphics[width=0.13\linewidth]{stefan} 
\end{tabular}} 
% Normally there will not be any pictures but we want
% these so that we can connect faces to names in the course
% We also want birthdates so that we can tell people with the same
% name apart
\date{}

\pagestyle{fancy}
\setlength{\headheight}{15pt}
\fancyhf{}
\lhead{DD2380 ai14} % DO NOT REMOVE!!!!
\rhead{A.K. Dhaka, D. Vidas, S. Annell} %% UPDATE WITH YOUR NAMES

\begin{document}

%\nocite{*}

\maketitle
\thispagestyle{fancy}

\begin{abstract}
Natural Language Generation is a Natural Language Processing task of
generating natural language. The problem is well known and well researched, but
the field is far from being fully explored. In this paper, we propose an idea of
building a text generator based on Hidden Markov Model. Hidden Markov Model
allows us to append words in a sentence that depend on a whole sequence until that point,
unlike other methods, such as n-gram, which use only a certain number of words
to predict the next one. We implemented three different types of Hidden Markov
Models for generation: unsupervised, supervised and chunked supervised. Three
different evaluators were used for comparing the performance of the generators with human written-sentences. 
Their respective performace suggested to us that Hidden Markov Model generators
work better than trigram generator over a small corpus and gives much more random texts whereas 
the latter generates better sentences when fed in with a large corpus.

\end{abstract}



\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}
Natural Language Generation (NLG) is the field of generating natural language
for a computer from a knowledge base or some information that the system can
understand. Some fields where the NLG is being applied are weather
forecast generation, summarize statistical data extracted from a database and
describing chains of reasoning performed by a system~\cite{nlgsystem01}.

This paper is about designing, implementing and comparing different methods of
generating random sentences when fed in by a specific corpus. The generated 
sentences should be as correct as possible grammatically and semantically,
which means that Natural Language Processing (NLP) needs to be implemented
for building the corpus and tagging / structuring the words, allowing our system
to form sentences that are meaningful and semantically as correct as possible.
The corpora used were of different sizes to test our system properly.

This is very interesting field of research and a good topic to study because it
incorporates several complicated tasks that are useful in wide variety of
applications. First of all, it is very difficult for a computer to understand
the content of a text, although there has been a lot of work done in this field. 
It is also hard for a computer to construct sentences that are correct
semantically because of the complexity of a natural languages, such as English
as mentioned in \cite{nlgScratch}. The evaluation of how correct a sentence is 
also very challenging and a field in its own. Furthermore, the 
aggravating circumstance that the language (both dictionary and the grammar)
is continuously evolving, makes it even harder to define a specific set of rules 
that apply all the time, considering that our datasets are written by numerous
authors and in different time instants. NLG systems broadly have three
subcomponents: text planning, sentence planning and surface realisation.
In text planning, content and target of the text are determined. During sentence
planning, syntactic and lexical means are ascertained whereas in realisation, 
actual words are strung together to give form to the basic structure which comes out
of sentence planning. 

It is also preferred if the results are benchmarked in some systematical way,
either manually (which is not preferred), semi-manually where the system might
give suggestions and the operator help the computer or it is done completely
automated.


\subsection{Contribution}
It is very hard for a computer to communicate with a human since the language is
very complex and it is hard to replicate. The language is one of the big
borders between computers and humans, which makes this field of research very
interesting. The method that we are presenting is a new and interesting way to
generate text focused on a topic using Hidden Markov Models (HMM). We used
three variants of HMM for this task: unsupervised HMM, supervised HMM and 
chunked supervised HMM. We also generated sentenced by trigrams and evaluated
the performance of all the generators with human written sentences using evaluators.

\subsection{Outline}
In Section~\ref{sec:relwork} previous work is recognized and the papers we have
researched to support our work is presented. Section~\ref{sec:method} contains
an overview on how we are going to solve the problem, and why we are choosing a
specific method and also how we are implementing the system. The result of the
implementation is presented in Section~\ref{sec:exps} and the whole paper is
summarized and concluded in Section~\ref{sec:summary}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related work}
\label{sec:relwork}

The field of NLG and NLP are two very large fields that are subfields of
artificial intelligence and computational linguistics. A lot of research has
been done in these fields, although the fields are far from fully explored. Some
of the earliest work in this field was performed in the 1970's by people such as
Goldman and Davey, when they helped to define the main problems in NLG 
as explained in \cite[p.~19-20]{buildingNLG}. They mention that NLG
started to be researched a lot during 1980's by people like McKeown and Appelt
that had a big impact on how the subsequent research was going to be performed on NLG.
The field had a big increase in the 1990's when many real-world
applications were made, such as the weather forecast system FOG.

The architecture and structuration of NLG systems is described
in~\cite{nlgsystem01}. They introduce six main tasks that need to be performed
when generating a text: content determination, discourse planning, sentence
aggregation, lexicalization, referring expression generation and linguistic
realisation. They also discuss the sub components the system needs to design and
the algorithms, using which the different subproblems can be solved. The main
use of this paper is the structure of the generation part of the system, but
also suggestions and pointers on how the arising problems can be solved.

Usage of HMMs in text generation is mentioned in~\cite{hmmnlg} where the model
is used in a task-oriented environment. Authors describe HMM generation process
to be very similar to POS (part of speech) tagging. States represent words, and
state sequences, word phrases. The model is designed from the corpus data
using ABL algorithm and it learns using the Baum-Welch algorithm.

One of the techniques which we have used, called chunking, is described
in~\cite{hmmchunk}. In this paper, chunking is used for parsing sentences. The
same method was used in our chunked supervised HMM implementation.

Some different methods for evaluating Natural Language Generation systems are
described in~\cite{poseval} and~\cite{bleueval}. Both papers compare automated
methods and human evaluation methods.~\cite{bleueval} also proposes methods such
as BLEU (Bilingual Evaluation Understudy), NIST (National Institute of Standards
and Technology) and Rogue which look at the words of the generated sentence, and
compare it to a reference. ~\cite{poseval} suggests a POS tagging method that
breaks up the sentences into POS tags, and compares it to a large reference
corpus. They both discuss the pros and cons of the evaluation systems and
suggest that a combination of automated methods should be used, since the
different automated methods cover different parts of the text.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hidden Markov Model}
\label{sec:method}

\subsection{N-gram}
N-gram models have been used for the task of NLG~\cite{nlgngram}. They
use the statistical property of co-occurrences to calculate the most likely word
to appear given previous ($n-1$) grams. We have used trigrams as one of the
generation techniques, but there are also some problems with n-gram models. One
of them is of the sparse data combinations as $n$ becomes greater. The other
problem which will be verified in the results section is that of non-randomness.
Trigram tends to pick full sentences out of the corpus and that is where HMMs
perform much better over trigrams. This problem with n-grams gets aggravated if
the training corpus is small in size. 


\subsection{Hidden Markov Models}
Hidden Markov Model (HMM) is a tool for modelling time series data.
It can be presented as a dynamic Bayesian network. In HMM, ``hidden''
stands for the states which are not directly visible to the observer~\cite{hmm}.
A certain HMM is fully defined by its:
\begin{itemize}
  \item transition matrix $A$,
  \item emission matrix $B$,
  \item initial state probability distribution $\pi$.
\end{itemize}

We propose to use HMMs for the surface generation task. HMMs have been used
with very good results in the fields of speech recognition and also in NLU
(Natural Language Understanding)~\cite{hmmsr}. The idea of HMM in this work is
to do something similar to the inverse of what POS tagging does. In POS
tagging, an input string of words is mapped to a hidden sequence of semantic POS
tags. The POS tags then can serve as the hidden states in the HMM model and the
words as the observations. The algorithm used for generating text with HMMs is
described by Algorithm~\ref{alg:hmm}.

In our work, we have used three types of HMMs for the text generation:
\textit{supervised, supervised with text chunking} and \textit{unsupervised}.

\begin{algorithm}
\caption{HMM Text Generation Algorithm}
\label{alg:hmm}
\begin{algorithmic}[1]
	\State $sequence \gets \emptyset$
	\State $state \gets \Call{pickRandomState}$
	\Repeat
		\State $emission \gets \Call{generateEmission}{state}$
		\State $sequence \gets \Call{append}{sequence, emission}$
		\State $state \gets \Call{stateTransition}{state}$
	\Until{enough} \\
	\Call{output}{sequence}
\end{algorithmic}
\end{algorithm}

\subsubsection{Supervised HMMs}
\label{sec:suphmm}
In the supervised HMMs, the POS tags are used as the states and the words and
punctuation of the corpus are treated as the emissions. NLTK's~\footnote{Natural
Language Toolkit - Open source library for NLG written in python} default POS
tagger was used for evaluating the POS tags in a sentence. The state-transition
matrix and emission matrix are then computed with them, using the supervised
learning algorithm. The algorithm is purely statistical and is described by the
following equations:
\begin{align}
\label{eq:hmm}
A_{ij} = \frac{S_{ij}}{\sum_{k} S_{ik}} \qquad
B_{i}(w) = \frac{E_{i}(w)}{\sum_{w'} E_{i}(w')} \qquad
\pi_{i} = \frac{S_{i}}{\sum_{k} S_{k}}
\end{align}
where $S_{ij}$ is the number of occurrences of transitions from state $i$ to
state $j$, $E_{i}(w)$ is the number of occurrences of the observation $w$ in the
state $i$ and $S_{i}$ is the number of occurrences of state $i$. The
parameters of the HMM (matrix $A$, matrix $B$ and vector $\pi$) are easily
computed using the equations~\ref{eq:hmm}.

\subsubsection{Supervised HMMs with Text Chunking}
This type of HMM is an extension of a supervised HMM described in
section~\ref{sec:suphmm}. Text chunking is a method of grouping words in a
sentence, in a way that they produce semantical units. It has been successfully 
applied for the task of shallow parsing~\cite{hmmchunk}. The idea is to enrich the
observations with information on the parts (chunks) of a sentence like noun
phrase (NP) and verb phrase (VP). For example, the sentence
\textit{``The quick brown fox jumps over the lazy dog.''} gets splitted into the
following chunks: \textit{``The quick brown fox'', ``jumps'', ``over'', ``a lazy
dog'', ``.''}. It can be clearly seen now that \textit{``The quick brown fox''}
is a single semantical unit.

In the chunked HMMs, we try to enrich the emissions with more
information about the semantics of a sentence and thus add chunk tags with POS
tags as hidden states. We also add chunks as emissions, which have previously
consisted of only words and punctuation. The learning algorithm is exactly the
same as the one used for the regular supervised HMMs.

\subsubsection{Unsupervised HMMs}
Unsupervised HMMs differ from supervised ones in several ways. The states are
implicit, the learner only knows the number of states but not what they
represent. The algorithm used for learning is Baum-Welch, which is a special
case of expectation maximization algorithm. Using this algorithm, we let the
learner find the patterns in the data and infer the states itself. The emissions
in unsupervised HMMs are the same as the ones in supervised HMMs.


\subsection {Performance measures for experiments}

As in other scientific fields, it is crucial to test how well our systems,
modules and algorithms work. This type of performance measure is called
evaluation. There are three techniques of evaluating NLG systems: task-based,
human ratings and metrics~\cite{evalnlg}. In our project we have used
metric evaluations.

Evaluation gives as an output the ``goodness'' of a sentence or a text. The
``goodness'' is defined by three characteristics: fluency, adequateness and
readability~\cite{evalmethods}. It is often a difficult task to assess each
characteristic.
Human ratings are based on opinions of individuals, rating the given
text. These types of evaluations are subjective and hard to measure. But the
main advantage is that the natural language is made for interactions between
people and we can evaluate texts more accurately than machines. In our project
we have spent a certain amount of time evaluating our models ourselves and with
the help of others to reduce the bias.

Metric evaluations are based on software evaluations of generated text. They are
more objective than human ratings, but also less reliable. Most evaluators
compare generated text with texts written by people to compute the score. In our
project, we have used three evaluators: BLEU, Linguistic checker and POS
sequence checker~\cite{autoeval}.

BLEU is an algorithm for evaluating the quality of a 
machine-translated text from one natural language to other. The way we have used it 
for our task is that, we compare the generated sentence with all the sentences of the
corpus in 4-grams which is considered the best number for evaluation ~\cite{bleueval}
and then calculate the maximum score out of it.


For evaluating the grammatical correctness of the generated sentence, we used the
open source tool named LANGUAGE TOOL. This software is quite clean and robust. It gives 
all the possible errors and their description when it is fed with a sentence. The number 
of errors in a sentence per the number of words in a sentence can be used as a measure
for evaluating the grammatical correctness of a sentence.

The third test used for evaluation is the POS tag sequence checker. This measure 
gives us a good estimate of how correct the sentence is semantically. The idea
is that a good sentence will have a particular sequence of POS tags. The
similarity between a generated sentence is calculated with all the sentences of
a corpus and the maximum score is taken as the final score. 

\subsection{Implementation}
\label{sec:impl}

We have used NLTK~\cite{nltk} very extensively. We used the built in brown
corpus and POS tagger trained on UPenn datasets. 
For the evaluation, the native implementation of BLEU in NLTK was used. 
For the linguistic checker evaluator, the open source tool LANGUAGE TOOL with a python
wrapper was used.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental results}
\label{sec:exps}

\subsection{Experimental setup}

We made two sets of tests with two different corpora as training data which were
different in length. The longer corpus we used was a H.C. Andersen book,
\textit{The Emperors new Clothes}, which was about 60 000 words long. The short
corpus was information about zebras that was pulled from Wikipedia which was about 2000
words long. When performing the tests, around 20 sentences were generated for
each corpus with each of the four systems and also 20 human written sentences
were produced. They were, then,  evaluated with the automated evaluation methods
which generated a score for each of the systems. The score of the human written
sentences was used as a performance reference. Finally, the results were plotted
and compared towards each other and the manually generated scores.
\subsection{Experiment}

As seen from the Figure~\ref{fig:longresults} the BLEU results are all very
close to the human results. Since human results are used as a benchmark value on 
what score a good sentence should get, and since all our results are close to
the human results, we have found that they are too unreliable. Because of
this, we have disregarded the BLEU score from our results.
It can be seen that the human generated sentences do not always have a score 
equal to 1 but close to it, which highlights the shortcomings of the evaluators we
have used and also because the evaluators have been trained on different corpora.

In the long corpus from Figure~\ref{fig:longresults}, the trigram performed 
the best since it received the highest LC and POS score, which co-relates with the
grammatics and the semantics of the sentence. The performance of the 
HMM systems were all similar. The system that performed better than the 
others was the Supervised HMM, as seen on the POS score.

The short corpus gave a lower score for trigrams and a higher score for all the 
HMM systems, where the supervised was the best. Overall, the score was 
higher and closer to the human score with a shorter corpus.
The results of the trigrams here are quite deceiving. The sentences are 
not true random, they are usually just a copy of the corpus. Which is also the 
nature of trigrams when dealing with a shorter corpus.

The chunking HMM systems were introduced to increase the performance of the
supervised HMM score. But even though the testing on the short corpus gives a
better result than the long corpus, the chunking HMM still performs worse than
the supervised HMM.
A problem that we could see when generating sentences with all our HMM systems
was that they generated too many punctuations and quotation marks.

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{resultsShort}
\includegraphics[width=0.8\linewidth]{results}
\caption{Experimental results when generating sentences with the four tested systems 
for a long corpus and a short corpus respectively, using the three automated
evaluation methods, and comparing it to the human results.}
\label{fig:longresults}
\end{figure}

\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary and Conclusions}
We found the task of generating sentences to be very difficult. When having an
open sentence that is not constricted at all, the possible ways to build a sentence
are almost endless. Using a story as a corpus and generating sentences is even
more difficult as it needs to model the linguistic tools, and there are much
more degrees of freedom.

The trigram system that we have designed worked better than the HMM systems for
a long corpus, but for the short corpus the trigrams just copied whole sentences
from the corpus thus performing very badly. The HMMs got a better score for the
shorter corpus because of the lesser occurrence of the punctuation and quotation
marks.
One of the reasons why it did not perform better could have been that we needed
to tune the HMM parameters better and have more test data. Furthermore, an
additional text filter might have improved the performance, either at the input
to the HMMs or output from the HMMs.
When we tried to improve our HMM results with chunking, it did not improve
performance as much that we would have hoped as seen in
Figure~\ref{fig:longresults}. But we do feel that chunking still is a good way
to increase the richness of the sentences.

NLG is a hard area, there are no trivial methods for generating sentences that
would be grammatically and semantically correct.
Nevertheless, it is a very interesting research area that needs to be mastered
before the computers become seamlessly integrated with human society.
\label{sec:summary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{ref}
\bibliographystyle{plain}


\end{document}